# 渐进复杂度 (Asymptotic Complexity)

在算法分析中，我们不关注特定硬件环境下的**绝对运行时间**（因为计算资源会随技术演进而变化），而是关注算法性能的**增长趋势**

## 核心命题

评价一个算法的优劣，需要从两个维度进行分析：

1. **时间维度** ⏳：一个算法需要多少时间才能完成？（Time Complexity）
    
2. **空间维度** 🧠：为了计算，算法需要额外占用多少内存量？（Space Complexity）
    
    >**说明**：空间复杂度通常指辅助空间（Auxiliary Space），即除输入数据存储空间外，算法临时分配的内存
    

---

##  渐进分析 (Asymptotic Analysis)

由于**处理器速度、内存访问延迟以及编译器优化**等因素在不同系统间差异显著，直接测量**绝对运行时间**（Absolute Running Time）往往缺乏可比性和普适性。因此，我们采用一种抽象的评估框架——**渐进分析**，以输入规模 $n$ 为变量，量化算法的资源消耗。

### 核心逻辑：从具体耗时到函数模型 

以时间复杂度为例，我们将算法运行所需的时间资源抽象为一个关于输入规模 $n$ 的函数 $T(n)$。

- **渐进效率 (Asymptotic Efficiency)**：重点考察当 $n$ 趋向于无穷大（$n \to \infty$）时， $T(n)$的增长趋势。这反映了算法处理大规模输入时的可扩展性（Scalability）。
    
- 忽略次要矛盾，抓住增长本质：
    
    在数学视角下，随着 $n$ 的增长，函数中高阶项的贡献将主导整体行为。
    
    - **忽略低阶项与常数因子**：当 $n$ 足够大时，常数系数（Constant Factor）和低阶项（Low-order Terms）对函数整体数值的影响将迅速衰减，直至忽略不计。
        
    - **关注主导项 (Dominating Term)**：鉴于上述事实，我们只关注运行时间增长的**量级（Order of Growth）**。
        
    
    > **示例**：对于 $T(n) = 0.5n^2 + 100n + 5000$，当 $n$ 达到百万级别时，$0.5n^2$ 将占据数值的绝大部分权重。因此，我们说该算法的渐进复杂度是 $O(n^2)$。
    

###  为什么“忽略”低阶项是合理的？

一个低阶算法（比如 $O(n \log n)$）在处理海量数据时，即便其常数项系数很大，最终表现也一定会优于一个系数很小的高阶算法（如 $O(n^2)$）。



---
## 渐进记号 (Asymptotic Notations) 

在正式介绍定义符号之前，我们需要介绍两个基本概念：

- **$f(n)$**：代表算法的**实际运行时间函数**。它通过计算算法执行的每一步操作得出，通常包含复杂的常数和低阶项（如 $f(n) = 3n^2 + 10n + 5$）。
    
- **$g(n)$**：代表我们要对比的参考函数形式（如 $n^2, n\log n$）。它去掉了繁琐的细节，只保留增长的量级，作为评估 $f(n)$ 的“尺子”。
    

### 1. **大 $O$ 记号 (Big-O)** —— $O(g(n))$ 

- **定义**：给出函数f(n)的**渐进上界 (Asymptotic Upper Bound)**。
    
    - **含义**：如果 $f(n) = O(g(n))$，则存在正常数 $c$ 和 $n_0$，使得对于所有 $n \ge n_0$，都有 $0 \le f(n) \le cg(n)$。
        
    - **深度理解**：大 $O$ 描述的是一种“天花板”。当输入规模 $n$ 足够大时，算法的运行时间**绝不会超过**这个量级。
        
- 例子：**插入排序的最坏情况**。
	    
    其运行时间函数 $f(n) = an^2 + bn + c$。我们可以说 $f(n) = O(n^2)$。即使我们说它也是 $O(n^3)$ 在数学上也是正确的，但通常我们寻求最贴切的上界。
	    
- 解释：提供性能的**最坏情况保证**，确保算法在极端输入下**不会超过**该量级。


### 2. **大 $\Omega$ 记号 (Big-Omega)** —— $\Omega(g(n))$ 

- **定义**：给出函数f(n)的**渐进下界 (Asymptotic Lower Bound)**。
    
    - **含义**：如果 $f(n) = \Omega(g(n))$，则存在正常数 $c$ 和 $n_0$，使得对于所有 $n \ge n_0$，都有 $0 \le cg(n) \le f(n)$。
        
- 例子：**任何需要遍历数组一遍的算法。**
	    
    无论如何优化，只要算法必须检查 $n$ 个元素，其运行时间 $f(n)$ 至少是 $n$ 的常数倍，即 $f(n) = \Omega(n)$。
	    
- 解释：界定算法的下限，反映任务固有的**最小资源需求**。
    

### 3. **大 $\Theta$ 记号 (Big-Theta)** —— $\Theta(g(n))$ 

- **定义**：给出函数f(n)的**渐进紧确界 (Asymptotically Tight Bound)**。
    
    - **含义**：$f(n) = \Theta(g(n))$ 当且仅当 $f(n) = O(g(n))$ 且 $f(n) = \Omega(g(n))$。
        
- 例子：归并排序 (Merge Sort) 的运行时间。
	    
	归并排序在最好、最坏和平均情况下的表现都是 $n \log n$ 级别的，因此其运行时间 $f(n) = \Theta(n \log n)$。
		
- 解释：精确刻画算法的典型行为，**上下界一致**。

- **说明**：：只有当一个算法的运行时间在**所有输入情况**下都处于同一量级时，才使用 $\Theta$。如果插入排序最坏是 $O(n^2)$ 但最好是 $\Omega(n)$，我们不能笼统地说它是 $\Theta(n^2)$。
	

### 4. **小 $o$ 记号 (Little-o)** —— $o(g(n))$ 

- **定义**：给出函数f(n)的**非渐进紧确上界**。
    
    - **含义**：对于**任意**正实数 $c > 0$，都存在常数 $n_0 > 0$，使得对于所有 $n \ge n_0$，都有 $0 \le f(n) < cg(n)$。
        
- 例子：**线性函数与平方函数的关系**。
	    
	$f(n) = 2n$ 是 $o(n^2)$，因为随着 $n$ 变大，$2n$ 相比于 $n^2$ 会变得极其微小。
	    
- 解释：它表示算法的实际耗时增速**严格低于**这个量级，不存在相等的可能。


### 5. **小 $\omega$ 记号 (Little-omega)** —— $\omega(g(n))$ 

- **定义**：给出函数的**非渐进紧确下界**。
    
    - **含义**：对于**任意**正实数 $c > 0$，都存在常数 $n_0 > 0$，使得对于所有 $n \ge n_0$，都有 $0 \le cg(n) < f(n)$。
        
- 例子：平方函数与对数函数的关系。
    
	$f(n) = n^2$ 是 $\omega(\log n)$，因为 $n^2$ 的增长速度远超 $\log n$。
    
- 解释：它表示算法的耗时增速**严格超过**了某个标准。

### 渐进符号综合对比表

| **符号**                    | **类比实数关系** | **含义 (当 n→∞)**           |
| ------------------------- | ---------- | ------------------------ |
| **$f(n) = O(g(n))$**      | $a \le b$  | $f$ 的增长**不快于** $g$       |
| **$f(n) = \Omega(g(n))$** | $a \ge b$  | $f$ 的增长**不慢于** $g$       |
| **$f(n) = \Theta(g(n))$** | $a = b$    | $f$ 与 $g$ **同阶** (增长率相同) |
| **$f(n) = o(g(n))$**      | $a < b$    | $f$ 增长**严格慢于** $g$       |
| **$f(n) = \omega(g(n))$** | $a > b$    | $f$ 增长**严格快于** $g$       |

---

## 复杂度的增长顺序

以下复杂度按增长速度从小到大排序：
![[Pasted image 20251226114051.png]]
<p align="right">—— <i>图表引用自 [邓俊辉] 的老师《数据结构(C++语言版)》课件</i></p>

### 常数复杂度
#### O（1）

#####  💖核心内容

- **本质**：操作次数与输入规模 $n$ 无关，始终为常数。
    
- **特性**：效率最高，具有最强的扩展性（Scalability）。
    
- **典型例子**：
    
    - **数组随机访问**：给定下标 $i$，直接计算内存地址并访问，耗时固定。
        
    - **哈希表查询**：在没有剧烈冲突的情况下，通过哈希函数定位元素。
        
    - **基本运算**：如 `a + b`、位运算、赋值语句等。
        

---

##### ⚠️ 陷阱与谬误 (Traps and Fallacies)

- **极大的常数也是 $O(1)$**：
		
    - **反直觉事实**：如果 $f(n) = 8888^{9999}$，尽管这个数值巨大，但只要它不随 $n$ 的增大而增大，它依然是 $O(1)$。
        
    - **问题本质**：渐进分析关注的是**变化趋势**，而非**绝对耗时**。
        
- **循环体也可以是 $O(1)$**：
	    
    - **反直觉事实**：
	    - 一个简单的例子：`for i from 1 to 10000`
		    
	    - 其他例子 ：`for (i=1; i<n; i=1<<1)`，这个复杂度T（n）=O($log^*n$), 是个常数级复杂度，在后续会深入讨论。
		    
	- **问题本质**：只要循环的次数是一个固定的常数，那么无论循环内做了什么，整体依然是 $O(1)$。
        
- **条件分支不影响量级**：
    
    - 代码中包含 `if-else` 分支并不会改变其常数时间的本质。**只要**每个分支内的操作都是 $O(1)$，整体就是 $O(1)$。
        
- **有函数调用的代码，也可以是$O(1)$**：
    
    - 即便代码调用了多个函数，只要调用栈深度和每层函数的操作与 $n$ 无关，它就是 $O(1)$。



#### $O(\log^* n)$ —— 迭代对数复杂度 (Iterated Logarithm)

##### 💖 核心内容

- **本质**：表示在结果小于或等于 1 之前，对一个数**连续进行对数运算**（$\log$）的**次数**。

- **数学定义**：
		$$
			\log^* n = 
			\begin{cases} 
			0 & \text{if } n \le 1 \\
			1 + \log^*(\log n) & \text{if } n > 1 
			\end{cases}
		 $$
	- 递归逻辑
	- 计算过程演示：

| $n$                | $\log_2 n$ | $\log_2(\log_2 n)$ | $\log_2(\log_2(\log_2 n))$ | $\log^* n$ (迭代次数) |
| :----------------- | :--------- | :----------------- | :------------------------- | :---------------- |
| $2$                | $1$        | $-$                | $-$                        | **1**             |
| $4$ ($2^2$)        | $2$        | $1$                | $-$                        | **2**             |
| $16$ ($2^4$)       | $4$        | $2$                | $1$                        | **3**             |
| $65536$ ($2^{16}$) | $16$       | $4$                | $2$                        | **4**             |
| $2^{65536}$        | $65536$    | $16$               | $4$                        | **5**             |

		    
- **特性**：增长速度**极其缓慢**。在现实宇宙的计算规模内，它几乎可以被看作是一个常数（对于目前已知的所有物理限制内的 $n$，$O(\log^* n) \le 5$）。
    
- **典型例子**：
    
    - **并查集（Union-Find）**：在使用“路径压缩”和“按秩合并”优化后，其单次操作的平摊时间复杂度为 $O(\alpha(n))$，其中 $\alpha$ 是反阿克曼函数。在绝大多数情况下，它的表现非常接近 $O(\log^* n)$。
        
    - **分布式着色问题**：在某些分布式算法中，将循环图的 $n$ 着色缩减为 3 着色需要 $O(\log^* n)$ 轮通信。
        

---

##### ⚠️ 陷阱与谬误 (Traps and Fallacies)

- **它不是 $O(\log n)$ 的变体**：
    
    - **直觉对比**：$\log n$ 随 $n$ 增加而增长（虽然慢），但 $\log^* n$ 几乎是“静止”的。
        
    - **数据规模**：
        
        - 如果 $n = 2^{65536}$（这是一个天文数字），$\log n = 65536$。
            
        - 而 $\log^* n$ 仅仅等于 **5**。
	        
- **并非真常数**：虽然在工程中常被视为常数，但理论上当 $n \to \infty$ 时，$\log^* n$ 也会趋于无穷

