# 🚀 渐进复杂度 (Asymptotic Complexity)

在算法的丛林中，我们不关注特定机器上的绝对运行秒数（因为 CPU 会变），我们关注的是**趋势**。

## 📝 核心命题

评价一个算法的优劣，需要回答两个问题：

1. **时间维度** ⏳：一个算法需要多少时间才能完成？（Time Complexity）
    
2. **空间维度** 🧠：为了计算，算法需要额外霸占多少内存？（Space Complexity）
    
    > **注意**：空间复杂度通常指**额外空间**（Auxiliary Space），即除去存储原始数据所需的空间外，算法运行过程中临时开辟的内存。
    

---

## 📈 渐进分析 (Asymptotic Analysis)

在评价算法时，由于 **CPU 频率、内存带宽及编译器优化** 等外部环境千差万别，测量**绝对运行时间**（Absolute Running Time）往往会陷入“因机而异”的泥潭。因此，我们需要一套脱离具体硬件的抽象评价体系——**渐进分析**。

### 核心逻辑：从“耗时”到“函数” 🛠️

我们将算法运行所需的时间资源抽象为一个关于输入规模 $n$ 的函数 $T(n)$。

- **渐进效率 (Asymptotic Efficiency)**：我们不仅关注 $n$ 在较小范围内的具体取值，**更**关注当 $n$ 趋向于无穷大（$n \to \infty$）时，算法运行时间的增长趋势。这衡量的是算法在处理**大规模数据**时的**扩展能力**（Scalability）。
    
- 忽略次要矛盾，抓住增长本质：
    
    在数学视角下，随着 $n$ 的增长，函数中的各项贡献度会发生剧变。
    
    - **忽略低阶项与常数因子**：当 $n$ 足够大时，常数系数（Constant Factor）和低阶项（Low-order Terms）对函数整体数值的影响将迅速衰减，直至忽略不计。
        
    - **锁定主导项 (Dominating Term)**：我们只关注运行时间增长的**量级（Order of Growth）**。
        
    
    > **示例**：对于 $T(n) = 0.5n^2 + 100n + 5000$，当 $n$ 达到百万级别时，$0.5n^2$ 将占据数值的绝大部分权重。因此，我们说该算法的渐进复杂度是 $O(n^2)$。
    

### 💡 为什么对于低阶项“忽略”是合理的？

在算法的世界里存在“量级压制”：一个低阶算法（比如 $O(n \log n)$）在处理海量数据时，即便其常数项系数很大，最终表现也一定会优于一个系数很小的高阶算法（如 $O(n^2)$）。

**渐进分析的本质，就是为了让我们从琐碎的实现细节中跳出来，去审视大数据时的“算法在处理体质”差异。** ⚖️

---
## 标记符号 (Asymptotic Notations) 🏷️

在正式定义符号之前，我们需要明确两个基本概念：

- **$f(n)$**：通常代表算法的**实际运行时间函数**。它通过计算算法执行的每一步操作得出，通常包含复杂的常数和低阶项（如 $f(n) = 3n^2 + 10n + 5$）。
    
- **$g(n)$**：代表我们要对比的**标准函数形式**（如 $n^2, n\log n$）。它去掉了繁琐的细节，只保留增长的量级，作为评估 $f(n)$ 的“尺子”。
    

### 1. **大 $O$ 记号 (Big-O)** —— $O(g(n))$ 👼

- **定义**：给出函数的**渐进上界 (Asymptotic Upper Bound)**。
    
    - **含义**：如果 $f(n) = O(g(n))$，则存在正常数 $c$ 和 $n_0$，使得对于所有 $n \ge n_0$，都有 $0 \le f(n) \le cg(n)$。
        
    - **深度理解**：大 $O$ 描述的是一种“天花板”。当输入规模 $n$ 足够大时，算法的运行时间**绝不会超过**这个量级。
        
- 例子：**插入排序的最坏情况**。
    
    其运行时间函数 $f(n) = an^2 + bn + c$。我们可以说 $f(n) = O(n^2)$。即使我们说它也是 $O(n^3)$ 在数学上也是正确的，但通常我们寻求最贴切的上界。
    
- **白话**：它是一个**保证**。无论输入数据多糟糕，我的耗时也就是这个水平，甚至可能更好。

- 记忆技巧：实际的时间函数抬头一看，**"Oh my god, $g(n)$ is so high!"** $\to$ **$O(n)$ my god!**
    

### 2. **大 $\Omega$ 记号 (Big-Omega)** —— $\Omega(g(n))$ ⚓

- **定义**：给出函数的**渐进下界 (Asymptotic Lower Bound)**。
    
    - **含义**：如果 $f(n) = \Omega(g(n))$，则存在正常数 $c$ 和 $n_0$，使得对于所有 $n \ge n_0$，都有 $0 \le cg(n) \le f(n)$。
        
- 例子：**任何需要遍历数组一遍的算法。**
    
    无论如何优化，只要算法必须检查 $n$ 个元素，其运行时间 $f(n)$ 至少是 $n$ 的常数倍，即 $f(n) = \Omega(n)$。
    
- **白话**：它是一个**底线**。它描述了算法“最快也就这样了”。

- 记忆方法： $\Omega$ 看起来像个船锚，实际函数像小船，**锚抛在下面**。它是“保底”的底线（下界）。
    

### 3. **大 $\Theta$ 记号 (Big-Theta)** —— $\Theta(g(n))$ 🛥️

- **定义**：给出函数的**渐进紧确界 (Asymptotically Tight Bound)**。
    
    - **含义**：$f(n) = \Theta(g(n))$ 当且仅当 $f(n) = O(g(n))$ 且 $f(n) = \Omega(g(n))$。
        
- 例子：归并排序 (Merge Sort) 的运行时间。
    
    归并排序在最好、最坏和平均情况下的表现都是 $n \log n$ 级别的，因此其运行时间 $f(n) = \Theta(n \log n)$。
    
- **关键纠正**：
    
    - **专业理解**：只有当一个算法的运行时间在**所有输入情况**下都处于同一量级时，才使用 $\Theta$。如果插入排序最坏是 $O(n^2)$ 但最好是 $\Omega(n)$，我们不能笼统地说它是 $\Theta(n^2)$。
        
- **白话**：它表示“我就在这个范围内跑，上下都锁死了”。

- 记忆技巧：$\Theta$的形状像个小船，那么就代表实际运行函数本身波动的范围。
    

### 4. **小 $o$ 记号 (Little-o)** —— $o(g(n))$ 🎈

- **定义**：给出函数的**非渐进紧确上界**。
    
    - **含义**：对于**任意**正实数 $c > 0$，都存在常数 $n_0 > 0$，使得对于所有 $n \ge n_0$，都有 $0 \le f(n) < cg(n)$。
        
- 例子：**线性函数与平方函数的关系**。
    
    $f(n) = 2n$ 是 $o(n^2)$，因为随着 $n$ 变大，$2n$ 相比于 $n^2$ 会变得极其微小。
    
- **白话**：它表示算法的实际耗时增速**严格低于**这个量级，不存在相等的可能。

- 记忆技巧：“o” 看起来像个圆圆的**氢气球**。它太轻了，永远飞在实际运行函数的头顶，且越飞越高，直到消失在视线尽头
    

### 5. **小 $\omega$ 记号 (Little-omega)** —— $\omega(g(n))$ 🌊

- **定义**：给出函数的**非渐进紧确下界**。
    
    - **含义**：对于**任意**正实数 $c > 0$，都存在常数 $n_0 > 0$，使得对于所有 $n \ge n_0$，都有 $0 \le cg(n) < f(n)$。
        
- 例子：平方函数与对数函数的关系。
    
    $f(n) = n^2$ 是 $\omega(\log n)$，因为 $n^2$ 的增长速度远超 $\log n$。
    
- **白话**：它表示算法的耗时增速**严格超过**了某个标准。

- 记忆技巧：$\omega$ 是个深海火山口，常年在海底，代表极大的下层落差。

### 📊 渐进符号综合对比表

|**符号**|**读作**|**类比实数关系**|**含义 (当 n→∞)**|
|---|---|---|---|
|**$f(n) = O(g(n))$**|Big-Oh|$a \le b$|$f$ 的增长**不快于** $g$|
|**$f(n) = \Omega(g(n))$**|Big-Omega|$a \ge b$|$f$ 的增长**不慢于** $g$|
|**$f(n) = \Theta(g(n))$**|Big-Theta|$a = b$|$f$ 与 $g$ **同阶** (增长率相同)|
|**$f(n) = o(g(n))$**|Little-oh|$a < b$|$f$ 增长**严格慢于** $g$|
|**$f(n) = \omega(g(n))$**|Little-omega|$a > b$|$f$ 增长**严格快于** $g$|

---

## 💡 复杂度的“食物链”

以下复杂度按增长速度从小到大排序：
![[Pasted image 20251226114051.png]]
<p align="right">—— <i>图表引用自 [邓俊辉] 的老师《数据结构(C++语言版)》课件</i></p>

### 常数复杂度
#### O（1）

#####  💖核心内容

- **本质**：操作次数与输入规模 $n$ 无关。
    
- **效率**：效率最高，具有最强的扩展性（Scalability）。
    
- **典型例子**：
    
    - **数组随机访问**：给定下标 $i$，直接计算内存地址并访问，耗时固定。
        
    - **哈希表查询**：在没有剧烈冲突的情况下，通过哈希函数定位元素。
        
    - **基本运算**：如 `a + b`、位运算、赋值语句等。
        

---

##### ⚠️ 陷阱与谬误 (Traps and Fallacies)

- **极大的常数也是 $O(1)$**：
		
    - **反直觉事实**：如果 $f(n) = 8888^{9999}$，尽管这个数值巨大，但只要它不随 $n$ 的增大而增大，它依然是 $O(1)$。
        
    - **问题本质**：渐进分析关注的是**变化趋势**，而非**绝对耗时**。
        
- **循环体也可以是 $O(1)$**：
	    
    - **反直觉事实**：
	    - 简单的例子：`for i from 1 to 10000`
		    
	    - 有难度的例子 ：`for (i=1; i<n; i=1<<1)`，这个复杂度T（n）=O($log^*n$), 是个常数级复杂度，在后续会深入讨论。
		    
	- **问题本质**：只要循环的次数是一个固定的常数，那么无论循环内做了什么，整体依然是 $O(1)$。
        
- **分支逻辑不影响量级**：
    
    - 代码中包含 `if-else` 分支并不会改变其常数时间的本质。只要每个分支内的操作都是 $O(1)$，整体就是 $O(1)$。
        
- **函数调用与底层优化**：
    
    - 即便代码调用了多个函数，只要调用栈深度和每层函数的操作与 $n$ 无关，它就是 $O(1)$。


#### O（$log^* n$）